{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTu8wRJRqvvi"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.nn import functional as F\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "id": "q5taOP6rq4Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7oeEhLmsg8L",
        "outputId": "f38ba49b-3297-499f-f236-7f38d5c42372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/drive/MyDrive/Variable-Renaming/result.xlsx')\n",
        "\n",
        "# Get the lists of method and their labels.\n",
        "methods = df[\"method\"].values\n",
        "labels = df[\"label\"].values"
      ],
      "metadata": {
        "id": "UHUoPK9gKIy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_method(method, label):\n",
        "  method_tokens = tokenizer(method, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
        "  label_tokens = tokenizer(label, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
        "  method_tokens[\"labels\"] = label_tokens.input_ids.detach().clone()\n",
        "  return method_tokens"
      ],
      "metadata": {
        "id": "1mGwlTbyLN1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = None\n",
        "\n",
        "for i in range(len(methods)):\n",
        "  method = methods[i].replace('[BLANK]', tokenizer.mask_token)\n",
        "  token = tokenize_method(method, labels[i])\n",
        "\n",
        "  if inputs is None:\n",
        "    inputs = token\n",
        "  else:\n",
        "    inputs[\"input_ids\"] = torch.cat([inputs[\"input_ids\"], token[\"input_ids\"]])\n",
        "    inputs[\"token_type_ids\"] = torch.cat([inputs[\"token_type_ids\"], token[\"token_type_ids\"]])\n",
        "    inputs[\"attention_mask\"] = torch.cat([inputs[\"attention_mask\"], token[\"attention_mask\"]])\n",
        "    inputs[\"labels\"] = torch.cat([inputs[\"labels\"], token[\"labels\"]])\n"
      ],
      "metadata": {
        "id": "HEOUkROOR34R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[\"labels\"][0]"
      ],
      "metadata": {
        "id": "up3KQhprdY-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[\"input_ids\"].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA8qfq1IwM3n",
        "outputId": "31e789a6-b087-45aa-ce8a-35a99830c2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4074, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MeditationsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)"
      ],
      "metadata": {
        "id": "KNutiLheUMpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MeditationsDataset(inputs)\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "URzEEtxLbZfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# and move our model over to the selected device\n",
        "model.to(device)\n",
        "# activate training mode\n",
        "model.train()"
      ],
      "metadata": {
        "id": "v66h4XAZbmAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "# initialize optimizer\n",
        "optim = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYw6gvYpbqI1",
        "outputId": "356cd61c-77da-4af3-99de-518619a4db06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # for our progress bar\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # setup loop with TQDM and dataloader\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for batch in loop:\n",
        "        # initialize calculated gradients (from prev step)\n",
        "        optim.zero_grad()\n",
        "        # pull all tensor batches required for training\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        # process\n",
        "        outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                        labels=labels)\n",
        "        # extract loss\n",
        "        loss = outputs.loss\n",
        "        # calculate loss for every parameter that needs grad update\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optim.step()\n",
        "        # print relevant info to progress bar\n",
        "        logits = outputs.logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "    if epoch / 5 == 0:\n",
        "      torch.save(model, '/content/drive/MyDrive/Variable-Renaming/model.pt')"
      ],
      "metadata": {
        "id": "wCMtqcuJbqQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, '/content/drive/MyDrive/Variable-Renaming/model.pt')"
      ],
      "metadata": {
        "id": "mUW1Hj1Gurtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Variable-Renaming/model_state_dict.pt')"
      ],
      "metadata": {
        "id": "GJMYXJrfvcJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = torch.load('/content/drive/MyDrive/Variable-Renaming/model.pt')"
      ],
      "metadata": {
        "id": "_kE5h21yvtub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "method = '''def action(user, reply, text):    [BLANK] = user.get_room_temp('rooms')    for (room_type, room_name) in [BLANK]:        loaded_room = roomloader.load_room(room_name, room_type, user)        if loaded_room.name == text or crypt(loaded_room.name) == text:            if random.random() < 0.1:                reply('Что-то пошло не так, ты увидел фезку пролетающую у тебя над головой. Ощущения будто был нарушен межпространственный континуум.')                user.open_room(reply)            else:                user.open_room(reply, room_type, room_name)            return    reply('Такого выбора тебе не давали.')'''\n",
        "text = method.replace('[BLANK]', tokenizer.mask_token)\n",
        "input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
        "input_cuda = {\n",
        "    'input_ids': input['input_ids'].cuda(),\n",
        "    'token_type_ids': input['token_type_ids'].cuda(),\n",
        "    'attention_mask': input['attention_mask'].cuda()\n",
        "}\n",
        "output = model1(**input_cuda)\n",
        "logits = output.logits\n",
        "softmax = F.softmax(logits, dim = -1)\n"
      ],
      "metadata": {
        "id": "RYhYs8hb0VkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the indices of the masked tokens in the input sequence\n",
        "mask_token_indices = torch.where(input_cuda['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "\n",
        "# Iterate over each mask token index to get predictions\n",
        "top_predictions_per_mask = []\n",
        "for mask_index in mask_token_indices:\n",
        "    # Get the predictions for the current masked token\n",
        "    masked_token_logits = logits[0, mask_index, :]\n",
        "\n",
        "    # Pick the top 5 candidate tokens for the masked position\n",
        "    top_5_candidates = torch.topk(masked_token_logits, k=10, dim=-1)\n",
        "\n",
        "    # Convert the predicted token IDs to the respective words\n",
        "    predicted_token_ids = top_5_candidates.indices.tolist()\n",
        "    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)\n",
        "\n",
        "    # Save the predictions\n",
        "    top_predictions_per_mask.append(predicted_tokens)\n",
        "\n",
        "# Display the top 5 predictions for each masked token\n",
        "for i, predictions in enumerate(top_predictions_per_mask):\n",
        "    print(f\"Mask {i+1} top predictions: {predictions}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZu71fZ9yv4W",
        "outputId": "6033f2ad-cc95-414b-c536-2cbf870ef7f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask 1 top predictions: ['i', 'n', 'x', 'p', 's', 't', 'range', 'r', 'm', 'y']\n",
            "Mask 2 top predictions: ['i', 'range', 'x', 'n', 's', 'y', 'r', 't', 'k', 'm']\n",
            "Mask 3 top predictions: ['i', 'x', 'range', 'r', 'p', 't', 'n', 'y', 'data', 's']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the indices of the masked tokens in the input sequence\n",
        "mask_token_indices = torch.where(input_cuda['input_ids'] == tokenizer.mask_token_id)[1]\n",
        "\n",
        "# Iterate over each mask token index to get predictions\n",
        "top_predictions_per_mask = []\n",
        "for mask_index in range(30):\n",
        "    # Get the predictions for the current masked token\n",
        "    masked_token_logits = logits[0, mask_index, :]\n",
        "\n",
        "    # Pick the top 5 candidate tokens for the masked position\n",
        "    top_5_candidates = torch.topk(masked_token_logits, k=5, dim=-1)\n",
        "\n",
        "    # Convert the predicted token IDs to the respective words\n",
        "    predicted_token_ids = top_5_candidates.indices.tolist()\n",
        "    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)\n",
        "\n",
        "    # Save the predictions\n",
        "    top_predictions_per_mask.append(predicted_tokens)\n",
        "\n",
        "# Display the top 5 predictions for each masked token\n",
        "for i, predictions in enumerate(top_predictions_per_mask):\n",
        "    print(f\"Mask {i+1} top predictions: {predictions}\")"
      ],
      "metadata": {
        "id": "iwQQQflb1Nw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIFyHp3usoeP",
        "outputId": "6595d2a1-8ac3-45b5-982a-986be77077d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 318, 30522])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the top tokens predicted for the mask positions\n",
        "mask_positions = (input_cuda['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "\n",
        "# Get the top 5 predictions for the mask positions\n",
        "predicted_token_ids = softmax[mask_positions].topk(5).indices.squeeze()"
      ],
      "metadata": {
        "id": "RbQRGHAjpJ0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Depending on the number of mask tokens and resulting shape, handle accordingly\n",
        "predicted_tokens = []\n",
        "for idx, position in enumerate(mask_positions[1]):\n",
        "    if predicted_token_ids.ndim == 1:  # Single mask token in the input\n",
        "        top_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)\n",
        "    else:  # More than one mask token in the input\n",
        "        top_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids[:, idx])\n",
        "    predicted_tokens.append(top_tokens)\n",
        "\n",
        "# predicted_tokens now contains the top 5 predictions for each mask position\n",
        "print(predicted_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QYLhYxYsFtG",
        "outputId": "d05a12be-4a76-4dc9-dbb1-03423fe34b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['path', 'if'], ['s', ',']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.nn import functional as F\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# and move our model over to the selected device\n",
        "model.to(device)\n",
        "# activate training mode\n",
        "model.train()\n",
        "\n",
        "from transformers import AdamW\n",
        "# initialize optimizer\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "from tqdm import tqdm  # for our progress bar\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # setup loop with TQDM and dataloader\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for batch in loop:\n",
        "        # initialize calculated gradients (from prev step)\n",
        "        optim.zero_grad()\n",
        "        # pull all tensor batches required for training\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        # process\n",
        "        outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                        labels=labels)\n",
        "        # extract loss\n",
        "        loss = outputs.loss\n",
        "        # calculate loss for every parameter that needs grad update\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optim.step()\n",
        "        # print relevant info to progress bar\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "    if epoch / 5 == 0:\n",
        "      torch.save(model, '/content/drive/MyDrive/Variable-Renaming/model.pt')"
      ],
      "metadata": {
        "id": "5exhzwU14Fql"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}